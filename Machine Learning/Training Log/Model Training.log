Training history

Plants.h5


Version 1.*
Using 8 classes
class_list = ["Black Nightshade", "Broccoli", "Cabbage", "Cucumber", "Grape Vine", "Pepper", "Potato", "Tomato"]

v1.0 - 13-05-2021 - Local Computer Linux
Using Eden library dataset at: https://edenlibrary.ai/datasets
Training with uneven dataset distribution (e.g. Cabbage 150 items, Tomato 400 items, Grape 800 items with 0.9 split)
Result: on 25 epoch
	Loss 0.25 Acc 0.92 - Val_Loss 0.28 Val_Acc 0.90 - Tested with previously used training images only get 3/10 Acc

v1.1 - 14-05-2021 - Local Computer Linux
Using Eden library dataset
Training with even dataset distribution (e.g. Each classes has 50 items of Trainining and 10 items of Validating randomly picked)
Result: on 25 epoch
	Loss 0.76 Acc 0.73 - Val_Loss 0.78 Val_Acc 0.66
	Tested with previously used training images get 8/10 Acc
	Tested with new hand-picked Google Images only get 2/10 Acc
	
v1.2 - 16-05-2021 - Local Computer Linux
Using Eden Library dataset combined with Google Images
Training with even dataset distribution: from Plants Dataset Resized and Shuffled 2.0.py
	Training = [250, 25] - Total training images each class : 275
	Validation = [50, 0] - Total validating images each class : 50
	Test = [21, 4] - Total test images each class : 25

	*The first value indicates dataset from google images and the second indicates dataset from eden library
Result: on 100 epoch
	The first 20 epoch Acc and Val_Acc increased, then Val_Acc stays at a constant of 0.5
	Loss 0.48 Acc 0.82 - Val_Loss 2.11 Val_Acc 0.52 - Tested with new and never been seen test images only get 3/10 Acc

Version 2.*
Using various class but with more accuracy


v2.0 - 19-05-2021 - Local Computer Linux
Using only 3 classes: ["Cabbage", "Cucumber", "Grape Vine"] for testing purposes
Using Filtered Google Images and from dreamstime web image search.
Training with split size of 0.8 of total images for training, 0.8 of (total - training) for validating, and the rest for test.
Result: on 100 epoch
	Loss 0.77 Acc 0.65 - Val_Loss 0.77 Val_Acc 0.69 
	And tested with test images (never-used before) with 8/10 Acc.
	Tested with new images downloaded from google also give good results

v2.1 - 22-05-2021 - Local Computer Linux
Using 6 classes: ["Broccoli", "Cabbage", "Cucumber", "Grape Vine", "Potato", "Tomato"]
Using Filtered Google Images and from dreamstime web image search.
Training with split size of 0.8 of total images for training, 0.8 of (total - training) for validating, and the rest for test.
Result: on 200 epoch
	Loss 0.92 Acc 0.63 - Val_Loss 0.91 Val_Acc 0.65 
	And tested with test images (never-used before) with 7/10 Acc.
	But Cabbage class is often mistaken as Broccoli,
	And Tomato class is often mistaken as Potato

v2.2 - 22-05-2021 - Local Computer Linux
Transfer Learning with previous model: v2.1
Freeze all layer but the output layer to be re-trained
Using Filtered Google Images and from dreamstime web image search.
Training with split size of 0.8 of total images for training, 0.8 of (total - training) for validating, and the rest for test.
Result: on 50 epoch
	Loss 0.91 Acc 0.66 - Val_Loss 0.63 Val_Acc 0.77 
	And tested with test images (never-used before) with 8/10 Acc.
	Cabbage class now performs better
	Tomato class stay inaccurate, so the dataset that looks like potato is deleted

Version 3.*
Using 10 classes
class_list = ["Broccoli", "Cabbage", "Carrot", "Chili", "Cucumber", "Grape Vine", "Onion", "Potato", "Spinach", "Tomato"]
Final model and version
Intended to be used on App

v3.0 - 23-05-2021 - Local Computer Linux
Using Filtered Google Images and from dreamstime web image search.
Training with split size of 0.8 of total images for training, 0.8 of (total - training) for validating, and the rest for test.
Result: on 250 epoch
	Loss 1.43 Acc 0.51 - Val_Loss 1.22 Val_Acc 0.57 
	And tested with test images (never-used before) with 6/10 Acc.
	A lot of mistake due to Spinach only showing leaves

v3.1 - 23-05-2021 - Local Computer Linux
Transfer Learning with previous model: v3.0
Using all layers to be trained again because freezing doesn't change much
Using Filtered Google Images and from dreamstime web image search.
Training with split size of 0.8 of total images for training, 0.8 of (total - training) for validating, and the rest for test.
Result: on 100 epoch
	Loss 1.40 Acc 0.49 - Val_Loss 1.03 Val_Acc 0.64 
	And tested with test images (never-used before) with 6/10 Acc.
	Still didn't do much change or maybe even worse
	Converted to TFlite format for experiment with the cloud
